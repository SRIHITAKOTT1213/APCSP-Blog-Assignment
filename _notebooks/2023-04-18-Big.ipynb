{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Jupyter Notebook\n",
    "\n",
    "\n",
    "- title: Big Idea 4 Notes\n",
    "- toc: true\n",
    "- permalink: /Big/Idea4\n",
    "- categories: [Other Projects]\n",
    "- comments: true"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sequential computing is a computational model in which operations are performed in order one at a time.\n",
    "\n",
    "- Parallel computing is a computational model where the program is broken into multiple smaller sequential computing operations, so of which are performed \n",
    "simultaneously.\n",
    "\n",
    "- Distributed computing is a computational model in which multiple devices are used to run a program\n",
    "\n",
    "- Comparing efficiency of solutions can be done by comparing the time it takes them to perform the same task\n",
    "\n",
    "- A sequential solution takes as long as the sum of all its steps\n",
    "\n",
    "- A prallel computing solution takes as long as its sequential tasks plus the longest of its parallel tasks.\n",
    "\n",
    "- The \"speedup\" of a parallel computing solution is measured in the time it took to complete the task sequentially divided by the time it took to complete the task when done in parallel.\n",
    "\n",
    "- Parallel computing consists of a parallel portion and a sequential portion\n",
    "\n",
    "- Solutions that use parallel computing can scale more effectively than solutions that use sequential computing\n",
    "\n",
    "- Distributed computing allows problems to be solved that could not be solved on a single computer because of either the processiong time or the storage needs involved.\n",
    "\n",
    "- Distributed computing allows much larger problems to be solved quicker than they could be solved using a single computer\n",
    "\n",
    "-  When increasing the use of parallel computing in a solution, the efficiency of the solution is still limited by the sequential portion. This means that at some point, adding parallel portions will be no longer meaningfully increase efficiency.\n",
    "\n",
    "- Parallel computing schedules tasks to be executed at the same time and is normally done on the same computer\n",
    "\n",
    "- Parallel computing is harware driven. an example of parallel computing could be a teacher who allows peer grading for assignments so all the assignments are graded at once by the students instead of the teacher doing all the grading herself.\n",
    "\n",
    "- Speedup Time is Sequential Total processing time/Parallel Total time needed\n",
    "\n",
    "- Distributed Computing is the sending of tasks from one computer to one or more others. It is a model in which multiple devices are used to run a program.\n",
    "\n",
    "- Examples of Distrubed Computing: \n",
    "    - Google Web Search: you ask for a search, and Google sends the request to its thousands of server in the background\n",
    "\n",
    "    - Web Sevices: standards on how computers can ask for task execution over the web. This includes protocols like SOAP, RSS, REST, etc.\n",
    "\n",
    "    - Beowolf Clusters: special software to group different computers together.\n",
    "\n",
    "- Distributed Computing NEEDS Network."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
